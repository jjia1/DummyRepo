{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a625cd27-aa19-4dab-857d-3a5272313614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Latency (s)</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens/sec</th>\n",
       "      <th>Peak RAM (MB)</th>\n",
       "      <th>CPU Time (s)</th>\n",
       "      <th>Device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarization</td>\n",
       "      <td>tiiuae/falcon-7B</td>\n",
       "      <td>6921720704</td>\n",
       "      <td>26404.27</td>\n",
       "      <td>109.560</td>\n",
       "      <td>1220</td>\n",
       "      <td>11.14</td>\n",
       "      <td>420.28</td>\n",
       "      <td>2925.04</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarization</td>\n",
       "      <td>tiiuae/falcon-7B (GPU)</td>\n",
       "      <td>6921720704</td>\n",
       "      <td>13202.13</td>\n",
       "      <td>7.008</td>\n",
       "      <td>1220</td>\n",
       "      <td>174.08</td>\n",
       "      <td>693.79</td>\n",
       "      <td>6.29</td>\n",
       "      <td>GPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarization</td>\n",
       "      <td>tiiuae/Falcon-H1-34B-Instruct</td>\n",
       "      <td>33642516224</td>\n",
       "      <td>128336.01</td>\n",
       "      <td>393.751</td>\n",
       "      <td>1222</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1526.61</td>\n",
       "      <td>9974.79</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarization</td>\n",
       "      <td>tiiuae/falcon-40b</td>\n",
       "      <td>41303293952</td>\n",
       "      <td>157559.56</td>\n",
       "      <td>1605.262</td>\n",
       "      <td>1220</td>\n",
       "      <td>0.76</td>\n",
       "      <td>802.23</td>\n",
       "      <td>27967.44</td>\n",
       "      <td>CPU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Task                     Model Name   Parameters  Model Size (MB)  \\\n",
       "0  Summarization               tiiuae/falcon-7B   6921720704         26404.27   \n",
       "1  Summarization         tiiuae/falcon-7B (GPU)   6921720704         13202.13   \n",
       "2  Summarization  tiiuae/Falcon-H1-34B-Instruct  33642516224        128336.01   \n",
       "3  Summarization              tiiuae/falcon-40b  41303293952        157559.56   \n",
       "\n",
       "   Latency (s)  Tokens  Tokens/sec  Peak RAM (MB)  CPU Time (s) Device  \n",
       "0      109.560    1220       11.14         420.28       2925.04    CPU  \n",
       "1        7.008    1220      174.08         693.79          6.29    GPU  \n",
       "2      393.751    1222        3.10        1526.61       9974.79    CPU  \n",
       "3     1605.262    1220        0.76         802.23      27967.44    CPU  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Updated Benchmark Results Table with Atom-7B (CPU), Falcon-7B (GPU), Falcon-34B (CPU), and Falcon-40B (CPU) ===\n",
    "benchmark_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Task\": \"Summarization\",\n",
    "        \"Model Name\": \"tiiuae/falcon-7B\",\n",
    "        \"Parameters\": 6_921_720_704,\n",
    "        \"Model Size (MB)\": 26404.27,\n",
    "        \"Latency (s)\": 109.560,\n",
    "        \"Tokens\": 1220,\n",
    "        \"Tokens/sec\": 11.14,\n",
    "        \"Peak RAM (MB)\": 420.28,\n",
    "        \"CPU Time (s)\": 2925.040,\n",
    "        \"Device\": \"CPU\"\n",
    "    },\n",
    "    {\n",
    "        \"Task\": \"Summarization\",\n",
    "        \"Model Name\": \"tiiuae/falcon-7B (GPU)\",\n",
    "        \"Parameters\": 6_921_720_704,\n",
    "        \"Model Size (MB)\": 13202.13,\n",
    "        \"Latency (s)\": 7.008,\n",
    "        \"Tokens\": 1220,\n",
    "        \"Tokens/sec\": 174.08,\n",
    "        \"Peak RAM (MB)\": 693.79,\n",
    "        \"CPU Time (s)\": 6.290,\n",
    "        \"Device\": \"GPU\"\n",
    "    },\n",
    "    {\n",
    "        \"Task\": \"Summarization\",\n",
    "        \"Model Name\": \"tiiuae/Falcon-H1-34B-Instruct\",\n",
    "        \"Parameters\": 33_642_516_224,\n",
    "        \"Model Size (MB)\": 128336.01,\n",
    "        \"Latency (s)\": 393.751,\n",
    "        \"Tokens\": 1222,\n",
    "        \"Tokens/sec\": 3.10,\n",
    "        \"Peak RAM (MB)\": 1526.61,\n",
    "        \"CPU Time (s)\": 9974.790,\n",
    "        \"Device\": \"CPU\"\n",
    "    },\n",
    "    {\n",
    "        \"Task\": \"Summarization\",\n",
    "        \"Model Name\": \"tiiuae/falcon-40b\",\n",
    "        \"Parameters\": 41_303_293_952,\n",
    "        \"Model Size (MB)\": 157559.56,\n",
    "        \"Latency (s)\": 1605.262,\n",
    "        \"Tokens\": 1220,\n",
    "        \"Tokens/sec\": 0.76,\n",
    "        \"Peak RAM (MB)\": 802.23,\n",
    "        \"CPU Time (s)\": 27967.440,\n",
    "        \"Device\": \"CPU\"\n",
    "    }\n",
    "])\n",
    "\n",
    "# === Display the table ===\n",
    "benchmark_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b954e-59d3-455d-a0bd-16025bad532a",
   "metadata": {},
   "source": [
    "Whatâ€™s the relationship between memory consumption and latency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf7183-80d3-4bac-a24d-d2cb590141aa",
   "metadata": {},
   "source": [
    "It would seem to me that memory consumption and latency are closely related. According to my benchmarks thus far, models that use higher amounts of RAM tend to see a rise in latency as well, concluding that a model's latency directly correlates with its memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afb903-ae74-4d5f-a8d5-3a24916017f5",
   "metadata": {},
   "source": [
    "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 23.49 GiB of which 1.89 GiB is free. Process 1424475 has 6.50 GiB memory in use. Including non-PyTorch memory, this process has 15.09 GiB memory in use. Of the allocated memory 14.79 GiB is allocated by PyTorch, and 14.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25ca43-e6b8-4fc5-b13e-0cdffe88e68f",
   "metadata": {},
   "source": [
    "This error was returned when I attempted to run a 34 billion parameter text generation model. This is due to the fact that the model being run has far too many parameters for the 24 GB GPU to handle, 33,642,516,224 to be precise. The GPU doesn't have enough memory to allocate towards this model's processing, thus resulting in a message indicating failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919ad6e-b104-4738-a946-c3079fa86c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
